{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3voBUlrPHt"
      },
      "source": [
        "# Exercice 4: connexion à l'API Mistral\n",
        "L’objectif de cet exercice est de développer un mini-assistant conversationnel en Python qui utilise l’API Mistral pour répondre à des questions ou effectuer des tâches simples (résumé de texte, traduction, génération de code, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIraI0h7rZRL"
      },
      "source": [
        "Avant de commencer à regarder et utiliser le code ci-dessous, il faut créer une clé api sur le site de Mistral. Sur [console.mistral.ai](https://console.mistral.ai), créer une clé API pour lechat. Il faudra créer un compte et souscrire à un plan expérimental (gratuit) pour l’obtenir. Copier cette clé dans la variable API_KEY."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TloYNCj1guyF"
      },
      "outputs": [],
      "source": [
        "%pip install mistralai\n",
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lzaIHpzgKsn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from textwrap import dedent\n",
        "\n",
        "# Remplace ces valeurs par celles de ton compte Mistral\n",
        "API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "API_KEY = \"entre ta clé Mistral ici\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": \"mistral-tiny\",  # Remplace par le modèle que tu veux utiliser\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Bonjour, comment ça va ?\"}],\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = requests.post(API_URL, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Lève une erreur si la requête échoue\n",
        "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Erreur lors de la requête : {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ChxE0Vrrv_"
      },
      "source": [
        "Et voila, c'est tout, tu parles avec le chat! Maintenant, tansforme le code ci-dessous pour qu'il soit simple à utiliser. Créer une classe contenant une méthode pour initialiser la connexion à l'API et une pour envoyer un message au LLM et et récupérer son retour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKseprC9r6Do"
      },
      "outputs": [],
      "source": [
        "def mistral_chat(\n",
        "    messages,\n",
        "    model=\"mistral-small-latest\",  # tu peux mettre \"mistral-tiny\" si tu veux moins cher/plus rapide\n",
        "    temperature=0.3,\n",
        "    max_tokens=512,\n",
        "    timeout=60,\n",
        "):\n",
        "    \"\"\"\n",
        "    messages: liste de dicts [{\"role\": \"system|user|assistant\", \"content\": \"...\"}]\n",
        "    Retour: texte (str) de la réponse assistant.\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.post(API_URL, headers=HEADERS, json=payload, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        # Format standard: choices[0].message.content\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        # Tente d'afficher le contenu d'erreur si possible\n",
        "        try:\n",
        "            err = r.json()\n",
        "        except Exception:\n",
        "            err = r.text if \"r\" in locals() else \"\"\n",
        "        raise RuntimeError(f\"HTTPError: {e}\\nServer response: {err}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise RuntimeError(f\"Request failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojXN3iwHr74B"
      },
      "source": [
        "L'objectif du reste de ce TP est maintenant de créer l'assistant en ajoutant des méthodes à cette classe.\n",
        "\n",
        "Il faut pouvoir:\n",
        "\n",
        "1. Résumer un texte long\n",
        "2. Traduire un paragraphe en français vers l'anglais (ou d'autres langues en paramètres)\n",
        "3. Générer du code Python pour une tâche donnée\n",
        "\n",
        "Toutes les fonctions doivent être sur différents exemples. D'autres tâches aux choix peuvent être ajoutées à l'assistant sur le même principe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqyF4YrotG0r"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text, model=\"mistral-small-latest\", max_words=120):\n",
        "    \"\"\"\n",
        "    Résume un texte long en français.\n",
        "    max_words: contrainte de longueur (approx).\n",
        "    \"\"\"\n",
        "    system = \"Tu es un assistant académique. Tu résumes de manière fidèle et concise, sans inventer.\"\n",
        "    user = dedent(f\"\"\"\n",
        "    Résume le texte suivant en ~{max_words} mots.\n",
        "    - Conserve les points clés.\n",
        "    - Pas de détails inutiles.\n",
        "    - Pas d'invention.\n",
        "\n",
        "    TEXTE:\n",
        "    {text}\n",
        "    \"\"\")\n",
        "\n",
        "    return mistral_chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        model=model,\n",
        "        temperature=0.2,\n",
        "        max_tokens=300,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "long_text = dedent(\"\"\"\n",
        "L’apprentissage profond a transformé le traitement automatique des langues en permettant de modéliser des dépendances complexes.\n",
        "Les modèles récurrents (RNN, LSTM, GRU) ont d’abord été largement utilisés pour traiter des séquences.\n",
        "Ensuite, l’attention et les Transformers ont amélioré la parallélisation et la capture de dépendances longues.\n",
        "Cependant, ces modèles nécessitent beaucoup de données et de calcul.\n",
        "En pratique, on doit aussi considérer la robustesse, les biais, et l’évaluation sur des données hors distribution.\n",
        "Enfin, le choix du modèle dépend du compromis entre performance, coût et contraintes d’implémentation.\n",
        "\"\"\").strip()\n",
        "\n",
        "print(summarize_text(long_text, max_words=90))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_text(\n",
        "    text, source_lang=\"français\", target_lang=\"anglais\", model=\"mistral-small-latest\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Traduit un texte d'une langue vers une autre.\n",
        "    \"\"\"\n",
        "    system = \"Tu es un traducteur professionnel. Tu fournis une traduction naturelle, fidèle, et fluide.\"\n",
        "    user = dedent(f\"\"\"\n",
        "    Traduis le texte suivant du {source_lang} vers le {target_lang}.\n",
        "    - Reste fidèle au sens.\n",
        "    - Adapte le style pour que ce soit naturel dans la langue cible.\n",
        "    - Ne rajoute pas d'explications, uniquement la traduction.\n",
        "\n",
        "    TEXTE:\n",
        "    {text}\n",
        "    \"\"\")\n",
        "\n",
        "    return mistral_chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        model=model,\n",
        "        temperature=0.2,\n",
        "        max_tokens=400,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph_fr = dedent(\"\"\"\n",
        "Dans cette expérience, nous évaluons un modèle séquentiel pour prédire le prochain caractère.\n",
        "Nous comparons plusieurs architectures (LSTM, GRU) et analysons la perte de validation afin d’éviter un surapprentissage.\n",
        "\"\"\").strip()\n",
        "\n",
        "print(translate_text(paragraph_fr, source_lang=\"français\", target_lang=\"anglais\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_python_code(task_description, model=\"mistral-small-latest\"):\n",
        "    \"\"\"\n",
        "    Génère du code Python (avec explications minimales) pour une tâche donnée.\n",
        "    Retourne UNIQUEMENT du code (pas de texte autour).\n",
        "    \"\"\"\n",
        "    system = dedent(\"\"\"\n",
        "    Tu es un assistant Python. Tu réponds uniquement avec du code Python exécutable.\n",
        "    - Pas de markdown, pas d'explication.\n",
        "    - Code clair, robuste, avec fonctions et docstrings si utile.\n",
        "    - Évite les dépendances non-standard sauf si demandé.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    user = dedent(f\"\"\"\n",
        "    Écris du code Python pour la tâche suivante:\n",
        "\n",
        "    {task_description}\n",
        "\n",
        "    Rappels:\n",
        "    - Retourne uniquement du code Python.\n",
        "    - Le code doit être complet et exécutable.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    return mistral_chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "        model=model,\n",
        "        temperature=0.1,\n",
        "        max_tokens=700,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task = \"\"\"\n",
        "Charger un fichier CSV (chemin fourni), afficher:\n",
        "- le nombre de lignes/colonnes\n",
        "- les 5 premières lignes\n",
        "- le nombre de valeurs manquantes par colonne\n",
        "Puis sauvegarder un nouveau CSV nettoyé où les lignes contenant au moins une valeur manquante sont supprimées.\n",
        "Le chemin du CSV d'entrée et de sortie doivent être des variables en début de script.\n",
        "\"\"\"\n",
        "\n",
        "code = generate_python_code(task)\n",
        "print(code)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
